{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The file for training the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-14T18:07:02.201668Z",
     "end_time": "2024-03-14T18:07:02.203359Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_args_teeth():\n",
    "    arguments = argparse.Namespace(epochs=10,\n",
    "                                   log_dir='logs/my_run',\n",
    "                                   image_size=224,\n",
    "                                   dir_for_saving_images=\"visualization_results\", #visualization results of the part prototypes\n",
    "                                   seed=42,\n",
    "                                   batch_size=64,\n",
    "                                   gpu_ids=\"\",\n",
    "                                   disable_cuda =None,\n",
    "                                   dataset=\"teeth\",\n",
    "                                   validation_size=0.,\n",
    "                                   num_workers=8,\n",
    "                                   weighted_loss=False,\n",
    "                                   batch_size_pretrain=128,\n",
    "                                   net=\"convnext_tiny_26\",\n",
    "                                   disable_pretrained=False,\n",
    "                                   num_features=0, #can be increased, this determines the number of prototypes, if set to 0 it will be same as backbone\n",
    "                                   bias=False,\n",
    "                                   lr=0.05,\n",
    "                                   lr_block=0.00005,\n",
    "                                   lr_net=0.0005,\n",
    "                                   optimizer=\"Adam\",\n",
    "                                   weight_decay=0.0,\n",
    "                                   epochs_pretrain=10,\n",
    "                                   state_dict_dir_net=\"\",\n",
    "                                   freeze_epoch=10,\n",
    "\n",
    "                                    )\n",
    "    return arguments\n",
    "\n",
    "arguments =get_args_teeth()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-14T18:07:03.870046Z",
     "end_time": "2024-03-14T18:07:03.887760Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log dir:  logs/my_run\n",
      "Device used:  cpu with id []\n",
      "Num classes (k) =  3 ['filling', 'impacted', 'implant'] etc.\n",
      "Classes:  {'filling': 0, 'impacted': 1, 'implant': 2}\n",
      "Number of prototypes:  768\n",
      "chosen network is convnext\n",
      "Classification layer initialized with mean 1.0001195669174194\n",
      "Output shape:  torch.Size([64, 768, 26, 26])\n",
      "\n",
      "Pretrain Epoch 1 with batch size 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch1:   0% 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters that require gradient:  31\n",
      "Align weight:  0.1 , U_tanh weight:  5.0 Class weight: 0.0\n",
      "Pretrain? True Finetune? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pretrain Epoch1:  54% 7/13 [15:24<13:30, 135.11s/it, L: 5.281, LA:6.43, LT:0.927, num_scores>0.1:2.8]"
     ]
    }
   ],
   "source": [
    "import main\n",
    "main.run_pipnet(arguments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Train a PIP-Net [-h] [--epochs EPOCHS] [--log_dir LOG_DIR]\n",
      "                       [--image_size IMAGE_SIZE]\n",
      "Train a PIP-Net: error: unrecognized arguments: -f /Users/mehdigreefhorst/Library/Jupyter/runtime/kernel-11bd5591-8724-475d-8035-8128a921fb68.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehdigreefhorst/Desktop/PIPNet/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def get_args_teeth_command() -> argparse.Namespace:\n",
    "    \"\"\"\n",
    "    this only works through the command line\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser('Train a PIP-Net')\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=5,\n",
    "                        help='The number of epochs PIP-Net should be trained (second training stage)')\n",
    "\n",
    "    parser.add_argument('--log_dir',\n",
    "                        type=str,\n",
    "                        default='./runs/run_pipnet',\n",
    "                        help='The directory in which train progress should be logged')\n",
    "\n",
    "    parser.add_argument('--image_size',\n",
    "                        type=int,\n",
    "                        default=512,\n",
    "                        help='Input images will be resized to --image_size x --image_size (square). Code only tested with 224x224, so no guarantees that it works for different sizes.')\n",
    "    arguments = parser.parse_args()\n",
    "    if len(arguments.log_dir.split('/'))>2:\n",
    "        if not os.path.exists(arguments.log_dir):\n",
    "            os.makedirs(arguments.log_dir)\n",
    "    return arguments"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-14T16:39:12.565794Z",
     "end_time": "2024-03-14T16:39:12.568524Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_args() -> argparse.Namespace:\n",
    "\n",
    "    parser = argparse.ArgumentParser('Train a PIP-Net')\n",
    "    parser.add_argument('--dataset',\n",
    "                        type=str,\n",
    "                        default='teeth',\n",
    "                        help='Data set on PIP-Net should be trained')\n",
    "    parser.add_argument('--validation_size',\n",
    "                        type=float,\n",
    "                        default=0.,\n",
    "                        help='Split between training and validation set. Can be zero when there is a separate test or validation directory. Should be between 0 and 1. Used for partimagenet (e.g. 0.2)')\n",
    "    parser.add_argument('--net',\n",
    "                        type=str,\n",
    "                        default='convnext_tiny_26',\n",
    "                        help='Base network used as backbone of PIP-Net. Default is convnext_tiny_26 with adapted strides to output 26x26 latent representations. Other option is convnext_tiny_13 that outputs 13x13 (smaller and faster to train, less fine-grained). Pretrained network on iNaturalist is only available for resnet50_inat. Options are: resnet18, resnet34, resnet50, resnet50_inat, resnet101, resnet152, convnext_tiny_26 and convnext_tiny_13.')\n",
    "    parser.add_argument('--batch_size',\n",
    "                        type=int,\n",
    "                        default=64,\n",
    "                        help='Batch size when training the model using minibatch gradient descent. Batch size is multiplied with number of available GPUs')\n",
    "    parser.add_argument('--batch_size_pretrain',\n",
    "                        type=int,\n",
    "                        default=128,\n",
    "                        help='Batch size when pretraining the prototypes (first training stage)')\n",
    "    parser.add_argument('--epochs',\n",
    "                        type=int,\n",
    "                        default=5,\n",
    "                        help='The number of epochs PIP-Net should be trained (second training stage)')\n",
    "    parser.add_argument('--epochs_pretrain',\n",
    "                        type=int,\n",
    "                        default = 10,\n",
    "                        help='Number of epochs to pre-train the prototypes (first training stage). Recommended to train at least until the align loss < 1'\n",
    "                        )\n",
    "    parser.add_argument('--optimizer',\n",
    "                        type=str,\n",
    "                        default='Adam',\n",
    "                        help='The optimizer that should be used when training PIP-Net')\n",
    "    parser.add_argument('--lr',\n",
    "                        type=float,\n",
    "                        default=0.05,\n",
    "                        help='The optimizer learning rate for training the weights from prototypes to classes')\n",
    "    parser.add_argument('--lr_block',\n",
    "                        type=float,\n",
    "                        default=0.0005,\n",
    "                        help='The optimizer learning rate for training the last conv layers of the backbone')\n",
    "    parser.add_argument('--lr_net',\n",
    "                        type=float,\n",
    "                        default=0.0005,\n",
    "                        help='The optimizer learning rate for the backbone. Usually similar as lr_block.')\n",
    "    parser.add_argument('--weight_decay',\n",
    "                        type=float,\n",
    "                        default=0.0,\n",
    "                        help='Weight decay used in the optimizer')\n",
    "    parser.add_argument('--disable_cuda',\n",
    "                        action='store_true',\n",
    "                        help='Flag that disables GPU usage if set')\n",
    "    parser.add_argument('--log_dir',\n",
    "                        type=str,\n",
    "                        default='./runs/run_pipnet',\n",
    "                        help='The directory in which train progress should be logged')\n",
    "    parser.add_argument('--num_features',\n",
    "                        type=int,\n",
    "                        default = 0,\n",
    "                        help='Number of prototypes. When zero (default) the number of prototypes is the number of output channels of backbone. If this value is set, then a 1x1 conv layer will be added. Recommended to keep 0, but can be increased when number of classes > num output channels in backbone.')\n",
    "    parser.add_argument('--image_size',\n",
    "                        type=int,\n",
    "                        default=224,\n",
    "                        help='Input images will be resized to --image_size x --image_size (square). Code only tested with 224x224, so no guarantees that it works for different sizes.')\n",
    "    parser.add_argument('--state_dict_dir_net',\n",
    "                        type=str,\n",
    "                        default='',\n",
    "                        help='The directory containing a state dict with a pretrained PIP-Net. E.g., ./runs/run_pipnet/checkpoints/net_pretrained')\n",
    "    parser.add_argument('--freeze_epochs',\n",
    "                        type=int,\n",
    "                        default = 10,\n",
    "                        help='Number of epochs where pretrained features_net will be frozen while training classification layer (and last layer(s) of backbone)'\n",
    "                        )\n",
    "    parser.add_argument('--dir_for_saving_images',\n",
    "                        type=str,\n",
    "                        default='visualization_results',\n",
    "                        help='Directoy for saving the prototypes and explanations')\n",
    "    parser.add_argument('--disable_pretrained',\n",
    "                        action='store_true',\n",
    "                        help='When set, the backbone network is initialized with random weights instead of being pretrained on another dataset).'\n",
    "                        )\n",
    "    parser.add_argument('--weighted_loss',\n",
    "                        action='store_true',\n",
    "                        help='Flag that weights the loss based on the class balance of the dataset. Recommended to use when data is imbalanced. ')\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help='Random seed. Note that there will still be differences between runs due to nondeterminism. See https://pytorch.org/docs/stable/notes/randomness.html')\n",
    "    parser.add_argument('--gpu_ids',\n",
    "                        type=str,\n",
    "                        default='',\n",
    "                        help='ID of gpu. Can be separated with comma')\n",
    "    parser.add_argument('--num_workers',\n",
    "                        type=int,\n",
    "                        default=8,\n",
    "                        help='Num workers in dataloaders.')\n",
    "    parser.add_argument('--bias',\n",
    "                        action='store_true',\n",
    "                        help='Flag that indicates whether to include a trainable bias in the linear classification layer.'\n",
    "                        )\n",
    "    parser.add_argument('--extra_test_image_folder',\n",
    "                        type=str,\n",
    "                        default='./experiments',\n",
    "                        help='Folder with images that PIP-Net will predict and explain, that are not in the training or test set. E.g. images with 2 objects or OOD image. Images should be in subfolder. E.g. images in ./experiments/images/, and argument --./experiments')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    if len(args.log_dir.split('/'))>2:\n",
    "        if not os.path.exists(args.log_dir):\n",
    "            os.makedirs(args.log_dir)\n",
    "\n",
    "\n",
    "    return args"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
